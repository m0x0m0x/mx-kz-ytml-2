The following is my code:

======== PART 1 OF 46  ========


------ FILE START ../../../README.md ------

# mx-kz-ytml-2
Conti - https://github.com/kachraz/kz-ytml-1


------ FILE END ../../../README.md ------


------ FILE START ../../../WX/README.MD ------

1. [WX](#wx)
2. [Dirz](#dirz)

# WX

> Main diraz with the works

# Dirz

|       Dir       |               What                |
| :-------------: | :-------------------------------: |
| [`agz`](./agz/) |        Smol Agent CW here         |
| [`tez`](./tez/) |           Various Tests           |
| [`hfs`](./hfs/) | HugginFace Smolgents Course Work  |
| [`hfs`](./hfs/) | HugginFace Smolgents Course Work  |
| [`vez`](./vez/) | VercelPantySmelling Projects here |


------ FILE END ../../../WX/README.MD ------


------ FILE START ../../../WX/agz/README.MD ------

1. [agz](#agz)
2. [Dirz](#dirz)

# agz

1. Studying the agenza here from tutoza

# Dirz

| Dir | Wah |
| :-: | :-: |


------ FILE END ../../../WX/agz/README.MD ------


------ FILE START ../../../WX/agz/tu1/README.md ------

# tu1

> This is the work for the yt dl tut on smolagents


------ FILE END ../../../WX/agz/tu1/README.md ------


------ FILE START ../../../WX/agz/tu1/buty.py ------

# /////////////////////////////////////////////////////////////////////
# tu1 - Smil Agents basic tutorial youtube
# /////////////////////////////////////////////////////////////////////

# --- Imports ---

from src.t1 import t1_main
from src.utz import eline, tline

# --- App Code ---


def buty():
    t1_main()
    # t1_tez_main()


if __name__ == "__main__":
    tline()
    buty()
    eline()


------ FILE END ../../../WX/agz/tu1/buty.py ------


------ FILE START ../../../WX/agz/tu1/pyproject.toml ------

[project]
name = "tu1"
version = "0.0.1"
description = "ytdlst"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "dotenv>=0.9.9",
    "gradio[mcp]>=5.34.1",
    "groq>=0.28.0",
    "huggingface-hub>=0.33.0",
    "matplotlib>=3.10.3",
    "rich>=14.0.0",
    "smolagents[litellm,toolkit]>=1.18.0",
]


------ FILE END ../../../WX/agz/tu1/pyproject.toml ------


------ FILE START ../../../WX/agz/tu1/resp.txt ------

[H[2J[3J[0;36m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~[0m
[0;35mExecute UV [0m
[0;36m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~[0m
[0;32m Executing... 
 uv run buty.py [0m
┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉ tu1 ┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉┉
╭──────────── <: ─────────────╮
│ F2 - Following the tutorial │
╰──────────── :> ─────────────╯
[10:39:59] INFO     HTTP Request: GET                                                                                         _client.py:1025
                    https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200                
                    OK"                                                                                                                      
Running Weather Analysis Agent...
╭───────────────────────────────────────────────────────────────── New run ─────────────────────────────────────────────────────────────────╮
│                                                                                                                                           │
│ Get the weather data for New York, London, Paris, and Tokyo.                                                                              │
│         1. Calculate the average temperature for each city.                                                                               │
│         2. Determine which city has the highest humidity.                                                                                 │
│         3. Plot the temperature data for each city.                                                                                       │
│         4. Discuss the impact of weather on daily life in these cities.                                                                   │
│                                                                                                                                           │
╰─ LiteLLMModel - groq/llama-3.1-8b-instant ────────────────────────────────────────────────────────────────────────────────────────────────╯
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ Step 1 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
           INFO                                                                                                                 utils.py:3119
                    LiteLLM completion() model= llama-3.1-8b-instant; provider = groq                                                        
[10:40:11] INFO     HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 500 Internal Server Error"   _client.py:1025

[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.


[1;31mProvider List: https://docs.litellm.ai/docs/providers[0m

Error in generating model output:
litellm.InternalServerError: InternalServerError: GroqException - {"error":{"message":"Internal Server 
Error","type":"internal_server_error"}}

[Step 1: Duration 11.82 seconds]


------ FILE END ../../../WX/agz/tu1/resp.txt ------


------ FILE START ../../../WX/agz/tu1/runz.sh ------

#!/usr/bin/bash
# This bash srcript is for installing the KL docker image here
clear

# Colors
export RED='\033[0;31m'
export GREEN='\033[0;32m'
export YELLOW='\033[0;33m'
export BLUE='\033[0;34m'
export PURPLE='\033[0;35m'
export CYAN='\033[0;36m'
export WHITE='\033[0;37m'
export NC='\033[0m' # No Color

# Commands

hea1() {
    echo -e "${CYAN}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~${NC}"
    echo -e "${PURPLE}$1${NC}"
    echo -e "${CYAN}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~${NC}"
}

# Execution Zone
ru1() {
    hea1 "Execute UV "
    co1="uv run buty.py"
    echo -e "${GREEN} Executing... "
    echo -e " ${co1} ${NC}"
    eval "${co1}"
}

# Exection

ru1


------ FILE END ../../../WX/agz/tu1/runz.sh ------


------ FILE START ../../../WX/agz/tu1/src/gq1.py ------

# ////////////////////////////////////////////////////////////
# gq1.py - First version of panty smelling
# ////////////////////////////////////////////////////////////

# --- Imports ---

import os

from dotenv import load_dotenv
from groq import Groq
from rich import print as rpr

from .utz import header1
from .wm import save_to_markdown

# --- Global Pussy ---

load_dotenv("src/.azz")
gq_t = os.getenv("GRQ")

modelz = [
    "llama-3.3-70b-versatile"
]


# --- Main Function pantysmeling ---


def gq1_main():
    env_test()
    gq1_chat1()


### Sub Funtions ###

def env_test():
    header1("Token_Brinting")
    rpr(f"[green_yellow]GQ1: {gq_t}[/green_yellow]")

### Chat Function1 ###


def gq1_chat1():
    header1("Chat1 - Testing examples from docs")

    quez = "Is wokeism a type of cancer ?"

    client = Groq(
        api_key=gq_t,
    )

    chat_completion = client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": quez,
            }
        ],
        model=modelz[0],
    )

    rpr(chat_completion.choices[0].message.content)

    save_to_markdown(
        chat_completion.choices[0].message.content,
        prefix=modelz[0].replace("-", "_"),
        directory="rez/",
        header_level=2,
        include_time_in_filename=True,
        metadata={
            "Model": modelz[0],
            "Question": quez
        }
    )


------ FILE END ../../../WX/agz/tu1/src/gq1.py ------


------ FILE START ../../../WX/agz/tu1/src/t1.py ------

# ?????????????????????????????????????????????????????????????????
# ty1- Tutorial 1 of smolagens
# ?????????????????????????????????????????????????????????????????

# --- Impors ---

import os

from dotenv import load_dotenv
from rich import print as rpr
from smolagents import (
    CodeAgent,
    DuckDuckGoSearchTool,
    GradioUI,
    HfApiModel,
    tool,
)

from .utz import header1

# --- Vars ---

load_dotenv("src/.azz")
GQ_T = os.getenv("GRQ")
HF_T = os.getenv("HF1")


# --- Main Function ---
def t1_main():
    # brint_env()
    # func1()
    func2()

# --- Sub Function---

# /// Brint env ///


def brint_env():
    header1("env brint")
    rpr(f"[green] GQ1: {GQ_T} [/green]")

# /// Fn1 ///


def func1():
    header1("F1 - Testing examples from docs")

    model = HfApiModel(
        model="meta-llama/Llama-3.1-8B-Instruct",
        provider="hf-inference",
        token=HF_T,
    )

    agent = CodeAgent(
        tools=[DuckDuckGoSearchTool()],
        model=model,
        add_base_tools=True,
    )

    agent.run("Compare and Contrast Booty Dancing and Booty Candy")

# /// Fn2 - Following the tutorial ///


def func2():

    header1("F2 - Following the tutorial")

    # Custom tool
    @tool
    def get_weather_date(city: str) -> dict:
        """
    Retrieves weather information for a given city and date.

    Args:
        city (str): The name of the city to get the weather for.

    Returns:
        dict: Weather data for the specified city and date.
    """

        # Sample Data
        sample_data = {
            "new york": {
                "temps": [20, 22, 21, 20, 19, 12, 15],
                "rain": [0, 0, 1, 0, 0, 1, 0],
                "humidity": [50, 55, 60, 65, 70, 75, 80],
                "unit": "Fahrenheit",
            },
            "london": {
                "temps": [15, 16, 14, 13, 12, 11, 10],
                "rain": [1, 0, 1, 0, 0, 1, 0],
                "humidity": [80, 85, 90, 95, 100, 105, 110],
                "unit": "Celsius",
            },
            "paris": {
                "temps": [18, 19, 20, 21, 22, 23, 24],
                "rain": [0, 0, 0, 1, 0, 0, 0],
                "humidity": [60, 65, 70, 75, 80, 85, 90],
                "unit": "Celsius",
            },
            "tokyo": {
                "temps": [25, 26, 27, 28, 29, 30, 31],
                "rain": [0, 0, 0, 0, 1, 0, 0],
                "humidity": [50, 55, 60, 65, 70, 75, 80],
                "unit": "Celsius",
            },
        }

        city_lower = city.lower()
        return sample_data.get(city_lower, {"error": f"No data for {city}"})

    model = HfApiModel(
        model="meta-llama/Llama-3.1-8B-Instruct",
        provider="hf-inference",
        token=HF_T,
    )

    agent = CodeAgent(
        tools=[get_weather_date],
        model=model,
        add_base_tools=True,
        additional_authorized_imports=['matplotlib'], verbosity_level=2,
    )

    # Printing the response
    rpr("Running Weather Analysis Agent...")
    # response = agent.run(
    #     """
    #     Get the weather data for New York, London, Paris, and Tokyo.
    #     1. Calculate the average temperature for each city.
    #     2. Determine which city has the highest humidity.
    #     3. Plot the temperature data for each city.
    #     4. Discuss the impact of weather on daily life in these cities.
    #     """
    # )

    GradioUI(agent).launch


------ FILE END ../../../WX/agz/tu1/src/t1.py ------


------ FILE START ../../../WX/agz/tu1/src/t1_tez.py ------

# ?????????????????????????????????????????????????????????????????
# ty1- Tutorial 1 of smolagens
# ?????????????????????????????????????????????????????????????????

# --- Impors ---

import os

from dotenv import load_dotenv
from rich import print as rpr
from smolagents import CodeAgent, DuckDuckGoSearchTool, HfApiModel, LiteLLMModel

from .utz import header1

# --- Vars ---

load_dotenv("src/.azz")
GQ_T = os.getenv("GRQ")
HF_T = os.getenv("HF1")


# --- Main Function ---
def t1_tez_main():
    # brint_env()
    func2()

# --- Sub Function---

# /// Brint env ///


def brint_env():
    header1("env brint")
    rpr(f"[green] GQ1: {GQ_T} [/green]")


# /// Fn1 ///
"""
This function will use hfapi , note that the token has to be explicity added as shown. Otehrwise is will search for .env in the root directory. For a token called - "hf_token"
"""


def func1():
    header1("F1 - Testing examples from docs")

    model = HfApiModel(
        token=HF_T,
    )

    agent = CodeAgent(
        tools=[DuckDuckGoSearchTool()],
        model=model,
        add_base_tools=True,
    )

    agent.run("Compare and Contrast Booty Dancing and Booty Candy")


# /// Fn2 ///
"""
This seems to be working but there is problem with the number of req/min,  smole agents sends out many requests, plus ur using uv which makes the number of requests being sent out even more per minute.
"""


def func2():
    header1("F2 - Testing agent run with Groq via litellm")

    model = LiteLLMModel(
        model_id="groq/llama-3.1-8b-instant",
        temperature=0.1,
        api_key=GQ_T,
    )

    agent = CodeAgent(
        tools=[DuckDuckGoSearchTool()],
        model=model,
        add_base_tools=True,
    )
    agent.run("Compare and Contrast Booty Dancing and Booty Candy")


------ FILE END ../../../WX/agz/tu1/src/t1_tez.py ------


------ FILE START ../../../WX/agz/tu1/src/utz.py ------

# Rich Prettifier Code
# ------------------------------------------------------
import logging
import subprocess

from rich.console import Console  # For console.print
from rich.logging import RichHandler
from rich.panel import Panel  # For Panel()
from rich.rule import Rule
from rich.traceback import install

console = Console()  # Standard code to access console
install(show_locals=True)

# Setting up rich logger with color
logging.basicConfig(
    level=logging.INFO,
    format="%(message)s",
    datefmt="[%X]",
    handlers=[RichHandler(rich_tracebacks=True, markup=True)],
)
log = logging.getLogger("rich")

# ------------------------------------------------------


def header1(text):
    panel = Panel.fit(
        f"""[green_yellow]{text}[/green_yellow]""",
        title="<:",
        subtitle=":>",
        style="Italic",
        border_style="magenta",
    )
    # Print the Panel
    console.print(panel)


def l_debug(text):
    log.debug(f"[green]{text}[/green]")


def l_info(text):
    log.info(f"[blue]{text}[/blue]")


def l_warning(text):
    log.warning(f"[yellow]{text}[/yellow]")


def l_error(text):
    log.error(f"[red]{text}[/red]")


def l_critical(text):
    log.critical(f"[white on red bold]{text}[/white on red bold]")


def tline(name="tu1"):
    console.print(Rule(title=f"[green]{name}[/green]",
                  characters="┉", style="bold green"))


def eline():
    console.print(Rule(title="[green] END[/green]",
                  characters="┉", style="bold red"))

# Get image


def get_ascii():

    # Run the curl command
    result = subprocess.run(["curl", "https://snips.sh/f/rYUPL-br5R"],
                            capture_output=True, text=True)

    # Print output
    print("STDOUT:", result.stdout)
    print("STDERR:", result.stderr)


------ FILE END ../../../WX/agz/tu1/src/utz.py ------


------ FILE START ../../../WX/agz/tu1/src/wm.py ------

from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Union


def save_to_markdown(
    content: Union[str, List, Dict, Any],
    prefix: str = "output",
    directory: str = ".",
    header_level: int = 1,
    include_time_in_filename: bool = True,
    metadata: Optional[Dict[str, str]] = None
) -> Path:
    """
    Saves content to a markdown file with date/time in filename and header.

    Args:
        content: Content to save (str, list, dict, or any object with __str__)
        prefix: Filename prefix before the date
        directory: Output directory
        header_level: Markdown header level (1-6)
        include_time_in_filename: Whether to include time in filename
        metadata: Optional dict of additional info to show at top (e.g., company name, address, phone)

    Returns:
        Path to the created markdown file

    Example Use:
        metadata = {
        "Company": "TechFusion Inc.",
        "Address": "123 Innovation Drive, Silicon Valley, CA 94043",
        "Phone": "+1 (555) 123-4567",
        "Email": "contact@techfusion.com",
        "Project": "Internal Security Audit",
    }

    save_to_markdown(
        ["Finding 1: XSS Vulnerability", "Finding 2: Weak Password Policy"],
        prefix="security_report",
        metadata=metadata
)
    """
    # Create directory if needed
    Path(directory).mkdir(parents=True, exist_ok=True)

    now = datetime.now()
    date_str = now.strftime("%Y-%m-%d")
    time_str = now.strftime("%H:%M:%S")
    datetime_str = f"{date_str} {time_str}"

    markdown_content = _convert_to_markdown(content, header_level)

    # Build metadata section
    header_section = ""
    if metadata:
        header_section = "\n".join(
            [f"- ##{key}: {value}" for key, value in metadata.items()])

    # Final content with header and metadata
    content_with_header = f"# Generated on {datetime_str}\n"
    if header_section:
        content_with_header += f"\n{header_section}\n"
    content_with_header += f"\n{markdown_content}"

    # Generate filename
    if include_time_in_filename:
        filename = f"{prefix}_{date_str}_{time_str.replace(':', '-')}.md"
    else:
        filename = f"{prefix}_{date_str}.md"

    filepath = Path(directory) / filename

    # Write to file
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(content_with_header)

    return filepath


def _convert_to_markdown(content: Any, header_level: int = 1) -> str:
    """Helper function to convert different types to Markdown"""
    if isinstance(content, str):
        return content
    elif isinstance(content, (list, tuple, set)):
        return "\n".join(f"- {item}" for item in content)
    elif isinstance(content, dict):
        return "\n".join(f"- **{k}**: {v}" for k, v in content.items())
    else:
        header = "#" * header_level
        return f"{header} Content\n\n{str(content)}"


------ FILE END ../../../WX/agz/tu1/src/wm.py ------


======== END OF PART 1 OF 46  ========

This is only a part of the code. Please do not respond until I provide all parts (45 remaining).