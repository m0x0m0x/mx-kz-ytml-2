The following is my code:

======== PART 1 OF 46  ========


------ FILE START ../../../README.md ------

# mx-kz-ytml-2
Conti - https://github.com/kachraz/kz-ytml-1


------ FILE END ../../../README.md ------


------ FILE START ../../../WX/README.MD ------

1. [WX](#wx)
2. [Dirz](#dirz)

# WX

> Main diraz with the works

# Dirz

|       Dir       |               What                |
| :-------------: | :-------------------------------: |
| [`agz`](./agz/) |        Smol Agent CW here         |
| [`tez`](./tez/) |           Various Tests           |
| [`hfs`](./hfs/) | HugginFace Smolgents Course Work  |
| [`hfs`](./hfs/) | HugginFace Smolgents Course Work  |
| [`vez`](./vez/) | VercelPantySmelling Projects here |


------ FILE END ../../../WX/README.MD ------


------ FILE START ../../../WX/agz/README.MD ------

1. [agz](#agz)
2. [Dirz](#dirz)

# agz

1. Studying the agenza here from tutoza

# Dirz

| Dir | Wah |
| :-: | :-: |


------ FILE END ../../../WX/agz/README.MD ------


------ FILE START ../../../WX/agz/tu1/README.md ------

# tu1

> This is the work for the yt dl tut on smolagents


------ FILE END ../../../WX/agz/tu1/README.md ------


------ FILE START ../../../WX/agz/tu1/buty.py ------

# /////////////////////////////////////////////////////////////////////
# tu1 - Smil Agents basic tutorial youtube
# /////////////////////////////////////////////////////////////////////

# --- Imports ---

from src.t1 import t1_main
from src.utz import eline, tline

# --- App Code ---


def buty():
    t1_main()
    # t1_tez_main()


if __name__ == "__main__":
    tline()
    buty()
    eline()


------ FILE END ../../../WX/agz/tu1/buty.py ------


------ FILE START ../../../WX/agz/tu1/pyproject.toml ------

[project]
name = "tu1"
version = "0.0.1"
description = "ytdlst"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "dotenv>=0.9.9",
    "gradio[mcp]>=5.34.1",
    "groq>=0.28.0",
    "huggingface-hub>=0.33.0",
    "matplotlib>=3.10.3",
    "rich>=14.0.0",
    "smolagents[litellm,toolkit]>=1.18.0",
]


------ FILE END ../../../WX/agz/tu1/pyproject.toml ------


------ FILE START ../../../WX/agz/tu1/resp.txt ------

[H[2J[3J[0;36m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~[0m
[0;35mExecute UV [0m
[0;36m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~[0m
[0;32m Executing... 
 uv run buty.py [0m
â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰ tu1 â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰â”‰
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ <: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ F2 - Following the tutorial â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ :> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[10:39:59] INFO     HTTP Request: GET                                                                                         _client.py:1025
                    https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200                
                    OK"                                                                                                                      
Running Weather Analysis Agent...
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ New run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚                                                                                                                                           â”‚
â”‚ Get the weather data for New York, London, Paris, and Tokyo.                                                                              â”‚
â”‚         1. Calculate the average temperature for each city.                                                                               â”‚
â”‚         2. Determine which city has the highest humidity.                                                                                 â”‚
â”‚         3. Plot the temperature data for each city.                                                                                       â”‚
â”‚         4. Discuss the impact of weather on daily life in these cities.                                                                   â”‚
â”‚                                                                                                                                           â”‚
â•°â”€ LiteLLMModel - groq/llama-3.1-8b-instant â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” Step 1 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
           INFO                                                                                                                 utils.py:3119
                    LiteLLM completion() model= llama-3.1-8b-instant; provider = groq                                                        
[10:40:11] INFO     HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 500 Internal Server Error"   _client.py:1025

[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.


[1;31mProvider List: https://docs.litellm.ai/docs/providers[0m

Error in generating model output:
litellm.InternalServerError: InternalServerError: GroqException - {"error":{"message":"Internal Server 
Error","type":"internal_server_error"}}

[Step 1: Duration 11.82 seconds]


------ FILE END ../../../WX/agz/tu1/resp.txt ------


------ FILE START ../../../WX/agz/tu1/runz.sh ------

#!/usr/bin/bash
# This bash srcript is for installing the KL docker image here
clear

# Colors
export RED='\033[0;31m'
export GREEN='\033[0;32m'
export YELLOW='\033[0;33m'
export BLUE='\033[0;34m'
export PURPLE='\033[0;35m'
export CYAN='\033[0;36m'
export WHITE='\033[0;37m'
export NC='\033[0m' # No Color

# Commands

hea1() {
    echo -e "${CYAN}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~${NC}"
    echo -e "${PURPLE}$1${NC}"
    echo -e "${CYAN}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~${NC}"
}

# Execution Zone
ru1() {
    hea1 "Execute UV "
    co1="uv run buty.py"
    echo -e "${GREEN} Executing... "
    echo -e " ${co1} ${NC}"
    eval "${co1}"
}

# Exection

ru1


------ FILE END ../../../WX/agz/tu1/runz.sh ------


------ FILE START ../../../WX/agz/tu1/src/gq1.py ------

# ////////////////////////////////////////////////////////////
# gq1.py - First version of panty smelling
# ////////////////////////////////////////////////////////////

# --- Imports ---

import os

from dotenv import load_dotenv
from groq import Groq
from rich import print as rpr

from .utz import header1
from .wm import save_to_markdown

# --- Global Pussy ---

load_dotenv("src/.azz")
gq_t = os.getenv("GRQ")

modelz = [
    "llama-3.3-70b-versatile"
]


# --- Main Function pantysmeling ---


def gq1_main():
    env_test()
    gq1_chat1()


### Sub Funtions ###

def env_test():
    header1("Token_Brinting")
    rpr(f"[green_yellow]GQ1: {gq_t}[/green_yellow]")

### Chat Function1 ###


def gq1_chat1():
    header1("Chat1 - Testing examples from docs")

    quez = "Is wokeism a type of cancer ?"

    client = Groq(
        api_key=gq_t,
    )

    chat_completion = client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": quez,
            }
        ],
        model=modelz[0],
    )

    rpr(chat_completion.choices[0].message.content)

    save_to_markdown(
        chat_completion.choices[0].message.content,
        prefix=modelz[0].replace("-", "_"),
        directory="rez/",
        header_level=2,
        include_time_in_filename=True,
        metadata={
            "Model": modelz[0],
            "Question": quez
        }
    )


------ FILE END ../../../WX/agz/tu1/src/gq1.py ------


------ FILE START ../../../WX/agz/tu1/src/t1.py ------

# ?????????????????????????????????????????????????????????????????
# ty1- Tutorial 1 of smolagens
# ?????????????????????????????????????????????????????????????????

# --- Impors ---

import os

from dotenv import load_dotenv
from rich import print as rpr
from smolagents import (
    CodeAgent,
    DuckDuckGoSearchTool,
    GradioUI,
    HfApiModel,
    tool,
)

from .utz import header1

# --- Vars ---

load_dotenv("src/.azz")
GQ_T = os.getenv("GRQ")
HF_T = os.getenv("HF1")


# --- Main Function ---
def t1_main():
    # brint_env()
    # func1()
    func2()

# --- Sub Function---

# /// Brint env ///


def brint_env():
    header1("env brint")
    rpr(f"[green] GQ1: {GQ_T} [/green]")

# /// Fn1 ///


def func1():
    header1("F1 - Testing examples from docs")

    model = HfApiModel(
        model="meta-llama/Llama-3.1-8B-Instruct",
        provider="hf-inference",
        token=HF_T,
    )

    agent = CodeAgent(
        tools=[DuckDuckGoSearchTool()],
        model=model,
        add_base_tools=True,
    )

    agent.run("Compare and Contrast Booty Dancing and Booty Candy")

# /// Fn2 - Following the tutorial ///


def func2():

    header1("F2 - Following the tutorial")

    # Custom tool
    @tool
    def get_weather_date(city: str) -> dict:
        """
    Retrieves weather information for a given city and date.

    Args:
        city (str): The name of the city to get the weather for.

    Returns:
        dict: Weather data for the specified city and date.
    """

        # Sample Data
        sample_data = {
            "new york": {
                "temps": [20, 22, 21, 20, 19, 12, 15],
                "rain": [0, 0, 1, 0, 0, 1, 0],
                "humidity": [50, 55, 60, 65, 70, 75, 80],
                "unit": "Fahrenheit",
            },
            "london": {
                "temps": [15, 16, 14, 13, 12, 11, 10],
                "rain": [1, 0, 1, 0, 0, 1, 0],
                "humidity": [80, 85, 90, 95, 100, 105, 110],
                "unit": "Celsius",
            },
            "paris": {
                "temps": [18, 19, 20, 21, 22, 23, 24],
                "rain": [0, 0, 0, 1, 0, 0, 0],
                "humidity": [60, 65, 70, 75, 80, 85, 90],
                "unit": "Celsius",
            },
            "tokyo": {
                "temps": [25, 26, 27, 28, 29, 30, 31],
                "rain": [0, 0, 0, 0, 1, 0, 0],
                "humidity": [50, 55, 60, 65, 70, 75, 80],
                "unit": "Celsius",
            },
        }

        city_lower = city.lower()
        return sample_data.get(city_lower, {"error": f"No data for {city}"})

    model = HfApiModel(
        model="meta-llama/Llama-3.1-8B-Instruct",
        provider="hf-inference",
        token=HF_T,
    )

    agent = CodeAgent(
        tools=[get_weather_date],
        model=model,
        add_base_tools=True,
        additional_authorized_imports=['matplotlib'], verbosity_level=2,
    )

    # Printing the response
    rpr("Running Weather Analysis Agent...")
    # response = agent.run(
    #     """
    #     Get the weather data for New York, London, Paris, and Tokyo.
    #     1. Calculate the average temperature for each city.
    #     2. Determine which city has the highest humidity.
    #     3. Plot the temperature data for each city.
    #     4. Discuss the impact of weather on daily life in these cities.
    #     """
    # )

    GradioUI(agent).launch


------ FILE END ../../../WX/agz/tu1/src/t1.py ------


------ FILE START ../../../WX/agz/tu1/src/t1_tez.py ------

# ?????????????????????????????????????????????????????????????????
# ty1- Tutorial 1 of smolagens
# ?????????????????????????????????????????????????????????????????

# --- Impors ---

import os

from dotenv import load_dotenv
from rich import print as rpr
from smolagents import CodeAgent, DuckDuckGoSearchTool, HfApiModel, LiteLLMModel

from .utz import header1

# --- Vars ---

load_dotenv("src/.azz")
GQ_T = os.getenv("GRQ")
HF_T = os.getenv("HF1")


# --- Main Function ---
def t1_tez_main():
    # brint_env()
    func2()

# --- Sub Function---

# /// Brint env ///


def brint_env():
    header1("env brint")
    rpr(f"[green] GQ1: {GQ_T} [/green]")


# /// Fn1 ///
"""
This function will use hfapi , note that the token has to be explicity added as shown. Otehrwise is will search for .env in the root directory. For a token called - "hf_token"
"""


def func1():
    header1("F1 - Testing examples from docs")

    model = HfApiModel(
        token=HF_T,
    )

    agent = CodeAgent(
        tools=[DuckDuckGoSearchTool()],
        model=model,
        add_base_tools=True,
    )

    agent.run("Compare and Contrast Booty Dancing and Booty Candy")


# /// Fn2 ///
"""
This seems to be working but there is problem with the number of req/min,  smole agents sends out many requests, plus ur using uv which makes the number of requests being sent out even more per minute.
"""


def func2():
    header1("F2 - Testing agent run with Groq via litellm")

    model = LiteLLMModel(
        model_id="groq/llama-3.1-8b-instant",
        temperature=0.1,
        api_key=GQ_T,
    )

    agent = CodeAgent(
        tools=[DuckDuckGoSearchTool()],
        model=model,
        add_base_tools=True,
    )
    agent.run("Compare and Contrast Booty Dancing and Booty Candy")


------ FILE END ../../../WX/agz/tu1/src/t1_tez.py ------


------ FILE START ../../../WX/agz/tu1/src/utz.py ------

# Rich Prettifier Code
# ------------------------------------------------------
import logging
import subprocess

from rich.console import Console  # For console.print
from rich.logging import RichHandler
from rich.panel import Panel  # For Panel()
from rich.rule import Rule
from rich.traceback import install

console = Console()  # Standard code to access console
install(show_locals=True)

# Setting up rich logger with color
logging.basicConfig(
    level=logging.INFO,
    format="%(message)s",
    datefmt="[%X]",
    handlers=[RichHandler(rich_tracebacks=True, markup=True)],
)
log = logging.getLogger("rich")

# ------------------------------------------------------


def header1(text):
    panel = Panel.fit(
        f"""[green_yellow]{text}[/green_yellow]""",
        title="<:",
        subtitle=":>",
        style="Italic",
        border_style="magenta",
    )
    # Print the Panel
    console.print(panel)


def l_debug(text):
    log.debug(f"[green]{text}[/green]")


def l_info(text):
    log.info(f"[blue]{text}[/blue]")


def l_warning(text):
    log.warning(f"[yellow]{text}[/yellow]")


def l_error(text):
    log.error(f"[red]{text}[/red]")


def l_critical(text):
    log.critical(f"[white on red bold]{text}[/white on red bold]")


def tline(name="tu1"):
    console.print(Rule(title=f"[green]{name}[/green]",
                  characters="â”‰", style="bold green"))


def eline():
    console.print(Rule(title="[green] END[/green]",
                  characters="â”‰", style="bold red"))

# Get image


def get_ascii():

    # Run the curl command
    result = subprocess.run(["curl", "https://snips.sh/f/rYUPL-br5R"],
                            capture_output=True, text=True)

    # Print output
    print("STDOUT:", result.stdout)
    print("STDERR:", result.stderr)


------ FILE END ../../../WX/agz/tu1/src/utz.py ------


------ FILE START ../../../WX/agz/tu1/src/wm.py ------

from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Union


def save_to_markdown(
    content: Union[str, List, Dict, Any],
    prefix: str = "output",
    directory: str = ".",
    header_level: int = 1,
    include_time_in_filename: bool = True,
    metadata: Optional[Dict[str, str]] = None
) -> Path:
    """
    Saves content to a markdown file with date/time in filename and header.

    Args:
        content: Content to save (str, list, dict, or any object with __str__)
        prefix: Filename prefix before the date
        directory: Output directory
        header_level: Markdown header level (1-6)
        include_time_in_filename: Whether to include time in filename
        metadata: Optional dict of additional info to show at top (e.g., company name, address, phone)

    Returns:
        Path to the created markdown file

    Example Use:
        metadata = {
        "Company": "TechFusion Inc.",
        "Address": "123 Innovation Drive, Silicon Valley, CA 94043",
        "Phone": "+1 (555) 123-4567",
        "Email": "contact@techfusion.com",
        "Project": "Internal Security Audit",
    }

    save_to_markdown(
        ["Finding 1: XSS Vulnerability", "Finding 2: Weak Password Policy"],
        prefix="security_report",
        metadata=metadata
)
    """
    # Create directory if needed
    Path(directory).mkdir(parents=True, exist_ok=True)

    now = datetime.now()
    date_str = now.strftime("%Y-%m-%d")
    time_str = now.strftime("%H:%M:%S")
    datetime_str = f"{date_str} {time_str}"

    markdown_content = _convert_to_markdown(content, header_level)

    # Build metadata section
    header_section = ""
    if metadata:
        header_section = "\n".join(
            [f"- ##{key}: {value}" for key, value in metadata.items()])

    # Final content with header and metadata
    content_with_header = f"# Generated on {datetime_str}\n"
    if header_section:
        content_with_header += f"\n{header_section}\n"
    content_with_header += f"\n{markdown_content}"

    # Generate filename
    if include_time_in_filename:
        filename = f"{prefix}_{date_str}_{time_str.replace(':', '-')}.md"
    else:
        filename = f"{prefix}_{date_str}.md"

    filepath = Path(directory) / filename

    # Write to file
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(content_with_header)

    return filepath


def _convert_to_markdown(content: Any, header_level: int = 1) -> str:
    """Helper function to convert different types to Markdown"""
    if isinstance(content, str):
        return content
    elif isinstance(content, (list, tuple, set)):
        return "\n".join(f"- {item}" for item in content)
    elif isinstance(content, dict):
        return "\n".join(f"- **{k}**: {v}" for k, v in content.items())
    else:
        header = "#" * header_level
        return f"{header} Content\n\n{str(content)}"


------ FILE END ../../../WX/agz/tu1/src/wm.py ------


======== END OF PART 1 OF 46  ========

This is only a part of the code. Please do not respond until I provide all parts (45 remaining).